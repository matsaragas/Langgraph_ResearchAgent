{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9087742f-ed8c-401e-98c4-39cceccfca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa6f8c-5a23-48ca-ba9f-29f9c8c3066c",
   "metadata": {},
   "source": [
    "### Load Datasets\n",
    "\n",
    "Load a collection AI research arxiv papers from hugging face. The papers have been chunked and information (chunk id with arxiv id, paper title, chunk text, post chunk id and arxiv id from reference papaer) from each chunk is provided in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3725c401-84e9-47be-87ef-3fb7b1d610b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jamescalam/ai-arxiv2-semantic-chunks\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb7b633-ef36-4fb0-8591-fa2ffd017b51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2401.04088#0',\n",
       " 'title': 'Mixtral of Experts',\n",
       " 'content': '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.',\n",
       " 'prechunk_id': '',\n",
       " 'postchunk_id': '2401.04088#1',\n",
       " 'arxiv_id': '2401.04088',\n",
       " 'references': ['1905.07830']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb2b6c-6dbb-4abe-b8f7-2cdee0e5a460",
   "metadata": {},
   "source": [
    "### Semantic Router\n",
    "\n",
    "Semantic Router is a superfast decision-making layer for your LLMs and agents. \n",
    "\n",
    "https://github.com/aurelio-labs/semantic-router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ce9365-8e5f-40b6-b4df-0f45dbd3c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from semantic_router.encoders import OpenAIEncoder\n",
    "pinecone_api_key = \"pcsk_H2epd_TG1DQBuiFhb4wDQR9RmLv4QKLXmiHADH86QHfx5uJGCF52kvPMs1YRL6AEb8fEb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b6d86d6-3ac4-48d2-b093-e2387bb5dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85eb433d-e3b8-4da2-93e8-d1773f3ad8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OpenAIEncoder(name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54023f4d-9936-434d-a2b2-26a7cc128437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28a8a93d-84c2-4248-b828-b4ff52123290",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "facf1956-b3f7-4ca8-b2e5-444958db24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c190e063-2c52-47bd-a20e-ba1c63797ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
